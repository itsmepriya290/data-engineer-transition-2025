# 🚀 Data Engineering Bootcamp Journey

Hi, I’m **Priya Kumari** 👋  
Currently working as an **E2E IP Planner (5+ years experience)**, transitioning into a **Senior Data Engineer** role.  
This repository is my **daily learning log + coding practice + projects** as I prepare for **top product-based companies (Google, Atlassian, etc.)**.  

---

## 🎯 Objective
To build strong expertise in:
- **Python (0 → Advanced)**  
- **SQL (0 → Advanced)**  
- **PySpark (0 → Advanced)**  
- **Data Engineering Concepts (ETL, pipelines, cloud, system design)**  
- **End-to-End Projects** that prove hands-on ability  

---

## 📅 Study Plan
| Week | Focus Area | Deliverables |
|------|------------|--------------|
| Week 1 | Python Basics | Scripts on variables, data types, loops, functions |
| Week 2 | Python Advanced | OOP, modules, error handling, file handling |
| Week 3 | SQL Basics | Joins, filtering, aggregations |
| Week 4 | SQL Advanced | Window functions, CTEs, optimization |
| Week 5 | PySpark Basics | RDD, DataFrames, transformations |
| Week 6 | PySpark Advanced | Optimizations, Spark SQL, partitioning |
| Week 7 | Cloud & Data Pipelines | GCP/AWS basics, Kafka, Airflow |
| Week 8 | Projects + Interview Prep | End-to-end projects, mock interview prep |

---

## 📂 Repository Structure
data-engineer-transition-2025/
├── Week1_Python/
│ ├── day1_basics.ipynb
│ ├── day2_strings.ipynb
│ └── ...
├── Week2_Python_Adv/
├── Week3_SQL/
├── Week4_SQL_Adv/
├── Week5_PySpark/
├── Week6_PySpark_Adv/
├── Week7_Projects/
└── README.md


---

## 🧠 Progress Tracker
- [x] Setup Python, GitHub, VSCode  
- [ ] Week 1 – Python Basics  
- [ ] Week 2 – Python Advanced  
- [ ] Week 3 – SQL Basics  
- [ ] Week 4 – SQL Advanced  
- [ ] Week 5 – PySpark Basics  
- [ ] Week 6 – PySpark Advanced  
- [ ] Week 7 – Cloud & Pipelines  
- [ ] Week 8 – Projects + Interview Prep  

---

## 📌 Daily Ritual
- Learn → Practice → Push code → Update tracker  
- Commit message format:  
  - Day X: [Topic] completed  

---

## 🚀 Upcoming Projects
- Data pipeline: Logs → Kafka → Spark → Data Warehouse  
- Real-time streaming with PySpark + Kafka  
- Cloud ETL pipeline (GCP BigQuery or AWS Redshift)  
- SQL-heavy analytics project  

---

## 👨‍💻 Author
**Priya Kumari**  
- 💼 Current Role: E2E IP Planner (5+ years)  
- 🎯 Target Role: Senior Data Engineer @ Google/Atlassian or similar  
- 🌐 LinkedIn: [www.linkedin.com/in/priyakumari2904](https://www.linkedin.com/in/priyakumari2904)  
- 🐙 GitHub: [itsmepriya290](https://github.com/itsmepriya290)  

---

⭐ If you’re a recruiter/engineer reviewing this repo → you’ll find my **consistent progress, problem-solving skills, and data engineering projects** here.  
